# Accuracy API

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Measure the accuracy and truthfulness of your LLM responses with a few lines of code.

> We are currently in closed beta.
> [Join our Discord](https://tally.so/r/3y0Lb0) to get early access and contribute

## Installation

Install the package with pip:

```bash
pip install accuracy-api
```

## Quick Start

```python
from accuracy_api import AccuracyAPI

# Initialize the API with your key
accuracy = AccuracyAPI(api_key="your_api_key")

# Check accuracy of a statement
check = accuracy.check("The capital of France is Paris")

if check.accuracy > 95:
    print("The response is accurate")
else:
    print("The response is not accurate")
```

## Full example

Add your own data and rank multiple responses generated by an LLM:

```python

accuracy.upload("my_refund_policy.pdf")
accuracy.conversation.upload("customer_payment_history.csv")

ranked_choices = accuracy.rank(openai.ChatCompletion.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "What is the refund policy for my plan?"}]
))

print(ranked_choices[0])
# {
#     "text": "You are eligible for a refund of $100",
#     "rank": 98,
#     "explanation": "The conversation has a lot of information about the customer's budget and preferences, and the response is consistent with that information."
#}
```

## How it works

The Accuracy API employs multiple validation techniques to estimate the accuracy of a string:

- **Semantic Similarity Analysis**: Evaluates semantic alignment between responses and prompts
- **Knowledge Base Verification**: Validates responses against established knowledge bases
- **Contextual Understanding**: Ensures consistency with conversation context
- **Chain of Thought Validation**: Verifies logical reasoning processes

## Usage

### Check call

This is the most basic call. It will use the world knowledge and the conversation context to validate the response.

```python
check = accuracy.check("The capital of France is Paris")
```

If you are using OpenAI and/or Anthropic, you can pass the `messages` directly:

```python
check = accuracy.check(messages=[{"role": "user", "content": "the capital of France is Paris"}])
```

### World Knowledge

By default, the Accuracy API will try to use its world knowledge to validate the response. If you are interested in latest-news or latest-research accuracy, you can change the settings in the account or conversation like so:

```python
# Enable for the whole account
accuracy.settings.set(
    latest_news=True,
    latest_research=True
)

# Enable for a specific conversation
accuracy.conversation.set(
    latest_news=True,
    latest_research=True,
    conversation_id=conversation_id
)
```

If you provide your own context, it will be used as a more authoritative source than the default world knowledge.

### Account-Level Context

Add reference context to your account:

```python
accuracy.account.upload("my_data.txt")
accuracy.account.upload("my_document.pdf")
```

Supported file formats:

- Text files (.txt)
- PDF documents (.pdf)
- CSV data (.csv)
- JSONL for semi-structured data (.jsonl)

Add simple text context to your account:

```python
accuracy.account.upload("a very long text string")
```

Connect your vector database:

```python
accuracy.account.connect("weaviate://localhost:8080")
```

We currently support Weaviate, Chroma, and Pinecone.

### Conversation-Specific Context

Add temporary context for specific conversations:

```python
# Start a new conversation
conversation_id = accuracy.conversation.start()

# Add conversation-specific data
accuracy.conversation.upload(
    "conversation_context.txt",
    id=conversation_id
)

# Add a message to the conversation
accuracy.conversation.add(
    conversation_id,
    [{"role": "user", "content": "I would like to buy a new car"}]
)

# Add simple text context to the conversation
accuracy.conversation.add(
    conversation_id,
    "the user has expressed a need for a new car"
)
```

Conversation-specific data expires after 14 days.

### Response Ranking

You can compare and rank multiple strings:

```python

# in this example, in the conversation and context: the customer has expressed a need for a new car

rankings = accuracy.rank([
    "the customer has expressed a need for a new car",
    "the customer has expressed a need for a used car",
    "the customer has expressed a need for a new car but he is on a budget and can't afford one now"
])
```

The API will return ranked results with explanations:

```python
[
    {
        "text": "the customer has expressed a need for a new car"
        "rank": 98,
        "explanation": "The conversation has a lot of information about the customer's budget and preferences, and the response is consistent with that information."
    },
    {
        "text": "the customer has expressed a need for a new car but he is on a budget and can't afford one now"
        "rank": 88,
        "explanation": "The customer has been thinking about the customer's budget and preferences, and the customer concluced that a new car is a good idea. The customer reports having enough money for a new car, albeit not a luxury one."
    },
      {
        "text": "the customer has expressed a need for a used car"
        "rank": 85,
        "explanation": "The conversation has a lot of information about the customer's budget and preferences, and the customer has been thinking about used cars, but the customer concluced that a used car is not a good idea."
    }
]
```

The `rank` method also works with the object returned by the most common LLM packages.

For example:

```python
from openai import OpenAI

client = OpenAI()

accuracy.rank(client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Did the customer express a need for a new car?"}]
))
```

The `rank` method will return the most accurate response first, given the different `choices` from OpenAI:

```python
[
    {
        "text": "Yes, the customer has expressed a need for a new car",
        "rank": 98,
        "index": 0,
        "message": {
            "role": "assistant",
            "content": "Yes, the customer has expressed a need for a new car"
        },
        "explanation": "The conversation has a lot of information about the user's budget and preferences, and the response is consistent with that information."
    },
    {
        "text": "Yes, the user has expressed a need for a new car but he is on a budget and can't afford one now",
        "rank": 88,
        "index": 1,
        "message": {
            "role": "assistant",
            "content": "Yes, the customer has expressed a need for a new car"
        },
        "explanation": "The conversation has a lot of information about the user's budget and preferences, and the response is consistent with that information."
    }
]
```

## Why Accuracy API?

[TruthfulQA](https://arxiv.org/abs/2109.07958) has shown that even the latest LLMs can provide inaccurate responses 30% of the time ([Claude Sonnet 3.5](https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf), [gpt-4o](https://openai.com/index/gpt-4o-system-card/)). While this may be acceptable for some use cases, it is not acceptable for others. Accuracy API aims to fix this issue at the cost of additional latency, compute and some additional lines of code.

## Documentation

We are currently in Closed Beta.
[Join our Discord](https://tally.so/r/3y0Lb0) to get early access and contribute

## License

This package is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
